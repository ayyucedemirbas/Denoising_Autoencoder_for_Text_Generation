{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPKbK4GRAVqrno9sDP5btb2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayyucedemirbas/Denoising_Autoencoder_for_Text_Generation/blob/main/Transformer_Based_Denoising_Autoencoder_for_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import requests\n",
        "import string\n",
        "import re\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "EX325Ls1RDj4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "text = requests.get(url).text"
      ],
      "metadata": {
        "id": "ka-sFdUvRE-3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text, min_word_freq=3):\n",
        "    words = re.findall(r\"\\w+[\\w']*\\w+\", text.lower())\n",
        "    word_counts = Counter(words)\n",
        "    vocab = ['<pad>', '<unk>', '<mask>'] + \\\n",
        "            [word for word, count in word_counts.items() if count >= min_word_freq]\n",
        "\n",
        "    word2idx = {word:i for i, word in enumerate(vocab)}\n",
        "    idx2word = {i:word for i, word in enumerate(vocab)}\n",
        "\n",
        "    data = []\n",
        "    for word in words:\n",
        "        if word in word2idx:\n",
        "            data.append(word2idx[word])\n",
        "        else:\n",
        "            data.append(word2idx['<unk>'])\n",
        "\n",
        "    return data, word2idx, idx2word, len(vocab)\n",
        "\n",
        "data, word2idx, idx2word, vocab_size = preprocess_text(text)"
      ],
      "metadata": {
        "id": "VcHkb-Q4RHbX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 32\n",
        "batch_size = 16\n",
        "embed_dim = 256\n",
        "num_heads = 4\n",
        "ff_dim = 512\n",
        "num_layers = 2\n",
        "noise_prob = 0.3\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "3QZE7_mHR-SO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_batches(data, batch_size, seq_length):\n",
        "    num_batches = len(data) // (batch_size * seq_length)\n",
        "    data = data[:num_batches * batch_size * seq_length]\n",
        "    data = torch.tensor(data).view(batch_size, -1)\n",
        "    return data\n",
        "\n",
        "def add_noise(batch):\n",
        "    device = batch.device\n",
        "    noisy_batch = batch.clone()\n",
        "\n",
        "    mask = torch.rand_like(noisy_batch.float(), device=device) < noise_prob\n",
        "    random_words = torch.randint(3, vocab_size, noisy_batch.shape, device=device)  # Skip special tokens\n",
        "\n",
        "    noisy_batch[mask] = random_words[mask]\n",
        "    return noisy_batch"
      ],
      "metadata": {
        "id": "lm1dUDyiSFJF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
        "        super().__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_dim, embed_dim)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output, _ = self.attention(x, x, x)\n",
        "        x = self.norm1(x + attn_output)\n",
        "        ff_output = self.ff(x)\n",
        "        return self.norm2(x + ff_output)\n",
        "\n",
        "class DenoisingTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, ff_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.positional_encoding = PositionalEncoding(embed_dim)\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, ff_dim) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x)\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "TV-ZDlykSLHy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DenoisingTransformer(vocab_size, embed_dim, num_heads, ff_dim, num_layers).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=word2idx['<pad>'])"
      ],
      "metadata": {
        "id": "2X36N9tqSTwR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_tensor = create_batches(data, batch_size, seq_length)\n",
        "num_batches = data_tensor.size(1) // seq_length"
      ],
      "metadata": {
        "id": "7eYCmf2fSXTz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(300):\n",
        "    total_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        inputs = data_tensor[:, i*seq_length:(i+1)*seq_length].to(device)\n",
        "        noisy_inputs = add_noise(inputs)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(noisy_inputs)\n",
        "        loss = criterion(outputs.view(-1, vocab_size), inputs.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss/num_batches:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGJ-U4rtSaWP",
        "outputId": "254e2597-61f2-42eb-8be2-0a58999761a0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 4.1265\n",
            "Epoch 2, Loss: 2.7979\n",
            "Epoch 3, Loss: 2.5466\n",
            "Epoch 4, Loss: 2.4298\n",
            "Epoch 5, Loss: 2.4010\n",
            "Epoch 6, Loss: 2.3636\n",
            "Epoch 7, Loss: 2.3366\n",
            "Epoch 8, Loss: 2.3303\n",
            "Epoch 9, Loss: 2.2930\n",
            "Epoch 10, Loss: 2.2908\n",
            "Epoch 11, Loss: 2.2782\n",
            "Epoch 12, Loss: 2.2730\n",
            "Epoch 13, Loss: 2.2496\n",
            "Epoch 14, Loss: 2.2434\n",
            "Epoch 15, Loss: 2.2385\n",
            "Epoch 16, Loss: 2.2266\n",
            "Epoch 17, Loss: 2.2191\n",
            "Epoch 18, Loss: 2.2100\n",
            "Epoch 19, Loss: 2.1934\n",
            "Epoch 20, Loss: 2.1818\n",
            "Epoch 21, Loss: 2.1918\n",
            "Epoch 22, Loss: 2.1731\n",
            "Epoch 23, Loss: 2.1565\n",
            "Epoch 24, Loss: 2.1581\n",
            "Epoch 25, Loss: 2.1502\n",
            "Epoch 26, Loss: 2.1520\n",
            "Epoch 27, Loss: 2.1465\n",
            "Epoch 28, Loss: 2.1320\n",
            "Epoch 29, Loss: 2.1320\n",
            "Epoch 30, Loss: 2.1210\n",
            "Epoch 31, Loss: 2.1001\n",
            "Epoch 32, Loss: 2.1027\n",
            "Epoch 33, Loss: 2.1088\n",
            "Epoch 34, Loss: 2.0980\n",
            "Epoch 35, Loss: 2.0832\n",
            "Epoch 36, Loss: 2.0891\n",
            "Epoch 37, Loss: 2.0697\n",
            "Epoch 38, Loss: 2.0736\n",
            "Epoch 39, Loss: 2.0679\n",
            "Epoch 40, Loss: 2.0470\n",
            "Epoch 41, Loss: 2.0461\n",
            "Epoch 42, Loss: 2.0437\n",
            "Epoch 43, Loss: 2.0482\n",
            "Epoch 44, Loss: 2.0413\n",
            "Epoch 45, Loss: 2.0200\n",
            "Epoch 46, Loss: 2.0115\n",
            "Epoch 47, Loss: 2.0197\n",
            "Epoch 48, Loss: 2.0074\n",
            "Epoch 49, Loss: 2.0078\n",
            "Epoch 50, Loss: 2.0033\n",
            "Epoch 51, Loss: 2.0066\n",
            "Epoch 52, Loss: 1.9886\n",
            "Epoch 53, Loss: 1.9821\n",
            "Epoch 54, Loss: 1.9698\n",
            "Epoch 55, Loss: 1.9808\n",
            "Epoch 56, Loss: 1.9634\n",
            "Epoch 57, Loss: 1.9638\n",
            "Epoch 58, Loss: 1.9519\n",
            "Epoch 59, Loss: 1.9506\n",
            "Epoch 60, Loss: 1.9443\n",
            "Epoch 61, Loss: 1.9405\n",
            "Epoch 62, Loss: 1.9376\n",
            "Epoch 63, Loss: 1.9291\n",
            "Epoch 64, Loss: 1.9259\n",
            "Epoch 65, Loss: 1.9236\n",
            "Epoch 66, Loss: 1.9068\n",
            "Epoch 67, Loss: 1.9122\n",
            "Epoch 68, Loss: 1.9174\n",
            "Epoch 69, Loss: 1.9023\n",
            "Epoch 70, Loss: 1.8881\n",
            "Epoch 71, Loss: 1.9032\n",
            "Epoch 72, Loss: 1.8885\n",
            "Epoch 73, Loss: 1.8847\n",
            "Epoch 74, Loss: 1.8713\n",
            "Epoch 75, Loss: 1.8755\n",
            "Epoch 76, Loss: 1.8659\n",
            "Epoch 77, Loss: 1.8717\n",
            "Epoch 78, Loss: 1.8635\n",
            "Epoch 79, Loss: 1.8613\n",
            "Epoch 80, Loss: 1.8499\n",
            "Epoch 81, Loss: 1.8600\n",
            "Epoch 82, Loss: 1.8440\n",
            "Epoch 83, Loss: 1.8522\n",
            "Epoch 84, Loss: 1.8314\n",
            "Epoch 85, Loss: 1.8251\n",
            "Epoch 86, Loss: 1.8395\n",
            "Epoch 87, Loss: 1.8249\n",
            "Epoch 88, Loss: 1.8223\n",
            "Epoch 89, Loss: 1.8367\n",
            "Epoch 90, Loss: 1.8204\n",
            "Epoch 91, Loss: 1.8181\n",
            "Epoch 92, Loss: 1.8158\n",
            "Epoch 93, Loss: 1.8083\n",
            "Epoch 94, Loss: 1.8105\n",
            "Epoch 95, Loss: 1.7945\n",
            "Epoch 96, Loss: 1.7994\n",
            "Epoch 97, Loss: 1.7996\n",
            "Epoch 98, Loss: 1.7935\n",
            "Epoch 99, Loss: 1.7997\n",
            "Epoch 100, Loss: 1.7936\n",
            "Epoch 101, Loss: 1.7894\n",
            "Epoch 102, Loss: 1.7796\n",
            "Epoch 103, Loss: 1.7829\n",
            "Epoch 104, Loss: 1.7783\n",
            "Epoch 105, Loss: 1.7876\n",
            "Epoch 106, Loss: 1.7825\n",
            "Epoch 107, Loss: 1.7872\n",
            "Epoch 108, Loss: 1.7810\n",
            "Epoch 109, Loss: 1.7676\n",
            "Epoch 110, Loss: 1.7736\n",
            "Epoch 111, Loss: 1.7669\n",
            "Epoch 112, Loss: 1.7646\n",
            "Epoch 113, Loss: 1.7529\n",
            "Epoch 114, Loss: 1.7728\n",
            "Epoch 115, Loss: 1.7645\n",
            "Epoch 116, Loss: 1.7494\n",
            "Epoch 117, Loss: 1.7507\n",
            "Epoch 118, Loss: 1.7460\n",
            "Epoch 119, Loss: 1.7378\n",
            "Epoch 120, Loss: 1.7472\n",
            "Epoch 121, Loss: 1.7489\n",
            "Epoch 122, Loss: 1.7414\n",
            "Epoch 123, Loss: 1.7411\n",
            "Epoch 124, Loss: 1.7346\n",
            "Epoch 125, Loss: 1.7382\n",
            "Epoch 126, Loss: 1.7412\n",
            "Epoch 127, Loss: 1.7323\n",
            "Epoch 128, Loss: 1.7341\n",
            "Epoch 129, Loss: 1.7374\n",
            "Epoch 130, Loss: 1.7407\n",
            "Epoch 131, Loss: 1.7278\n",
            "Epoch 132, Loss: 1.7257\n",
            "Epoch 133, Loss: 1.7145\n",
            "Epoch 134, Loss: 1.7249\n",
            "Epoch 135, Loss: 1.7226\n",
            "Epoch 136, Loss: 1.7093\n",
            "Epoch 137, Loss: 1.7048\n",
            "Epoch 138, Loss: 1.7076\n",
            "Epoch 139, Loss: 1.7159\n",
            "Epoch 140, Loss: 1.7111\n",
            "Epoch 141, Loss: 1.7154\n",
            "Epoch 142, Loss: 1.7063\n",
            "Epoch 143, Loss: 1.6992\n",
            "Epoch 144, Loss: 1.7127\n",
            "Epoch 145, Loss: 1.7129\n",
            "Epoch 146, Loss: 1.7059\n",
            "Epoch 147, Loss: 1.7056\n",
            "Epoch 148, Loss: 1.6947\n",
            "Epoch 149, Loss: 1.6998\n",
            "Epoch 150, Loss: 1.7147\n",
            "Epoch 151, Loss: 1.7063\n",
            "Epoch 152, Loss: 1.6937\n",
            "Epoch 153, Loss: 1.6917\n",
            "Epoch 154, Loss: 1.7029\n",
            "Epoch 155, Loss: 1.6871\n",
            "Epoch 156, Loss: 1.6882\n",
            "Epoch 157, Loss: 1.6938\n",
            "Epoch 158, Loss: 1.6902\n",
            "Epoch 159, Loss: 1.6769\n",
            "Epoch 160, Loss: 1.6965\n",
            "Epoch 161, Loss: 1.7012\n",
            "Epoch 162, Loss: 1.6854\n",
            "Epoch 163, Loss: 1.6864\n",
            "Epoch 164, Loss: 1.6911\n",
            "Epoch 165, Loss: 1.6856\n",
            "Epoch 166, Loss: 1.6782\n",
            "Epoch 167, Loss: 1.6897\n",
            "Epoch 168, Loss: 1.6921\n",
            "Epoch 169, Loss: 1.6724\n",
            "Epoch 170, Loss: 1.6596\n",
            "Epoch 171, Loss: 1.6663\n",
            "Epoch 172, Loss: 1.6771\n",
            "Epoch 173, Loss: 1.6662\n",
            "Epoch 174, Loss: 1.6711\n",
            "Epoch 175, Loss: 1.6815\n",
            "Epoch 176, Loss: 1.6739\n",
            "Epoch 177, Loss: 1.6545\n",
            "Epoch 178, Loss: 1.6624\n",
            "Epoch 179, Loss: 1.6785\n",
            "Epoch 180, Loss: 1.6742\n",
            "Epoch 181, Loss: 1.6572\n",
            "Epoch 182, Loss: 1.6706\n",
            "Epoch 183, Loss: 1.6624\n",
            "Epoch 184, Loss: 1.6712\n",
            "Epoch 185, Loss: 1.6546\n",
            "Epoch 186, Loss: 1.6599\n",
            "Epoch 187, Loss: 1.6593\n",
            "Epoch 188, Loss: 1.6591\n",
            "Epoch 189, Loss: 1.6638\n",
            "Epoch 190, Loss: 1.6624\n",
            "Epoch 191, Loss: 1.6675\n",
            "Epoch 192, Loss: 1.6527\n",
            "Epoch 193, Loss: 1.6605\n",
            "Epoch 194, Loss: 1.6405\n",
            "Epoch 195, Loss: 1.6477\n",
            "Epoch 196, Loss: 1.6524\n",
            "Epoch 197, Loss: 1.6480\n",
            "Epoch 198, Loss: 1.6448\n",
            "Epoch 199, Loss: 1.6592\n",
            "Epoch 200, Loss: 1.6427\n",
            "Epoch 201, Loss: 1.6398\n",
            "Epoch 202, Loss: 1.6460\n",
            "Epoch 203, Loss: 1.6385\n",
            "Epoch 204, Loss: 1.6483\n",
            "Epoch 205, Loss: 1.6416\n",
            "Epoch 206, Loss: 1.6438\n",
            "Epoch 207, Loss: 1.6472\n",
            "Epoch 208, Loss: 1.6338\n",
            "Epoch 209, Loss: 1.6312\n",
            "Epoch 210, Loss: 1.6414\n",
            "Epoch 211, Loss: 1.6380\n",
            "Epoch 212, Loss: 1.6343\n",
            "Epoch 213, Loss: 1.6243\n",
            "Epoch 214, Loss: 1.6255\n",
            "Epoch 215, Loss: 1.6437\n",
            "Epoch 216, Loss: 1.6325\n",
            "Epoch 217, Loss: 1.6182\n",
            "Epoch 218, Loss: 1.6294\n",
            "Epoch 219, Loss: 1.6316\n",
            "Epoch 220, Loss: 1.6146\n",
            "Epoch 221, Loss: 1.6302\n",
            "Epoch 222, Loss: 1.6346\n",
            "Epoch 223, Loss: 1.6265\n",
            "Epoch 224, Loss: 1.6275\n",
            "Epoch 225, Loss: 1.6279\n",
            "Epoch 226, Loss: 1.6142\n",
            "Epoch 227, Loss: 1.6244\n",
            "Epoch 228, Loss: 1.6285\n",
            "Epoch 229, Loss: 1.6250\n",
            "Epoch 230, Loss: 1.6229\n",
            "Epoch 231, Loss: 1.6110\n",
            "Epoch 232, Loss: 1.6294\n",
            "Epoch 233, Loss: 1.6116\n",
            "Epoch 234, Loss: 1.6205\n",
            "Epoch 235, Loss: 1.6161\n",
            "Epoch 236, Loss: 1.6244\n",
            "Epoch 237, Loss: 1.6223\n",
            "Epoch 238, Loss: 1.6228\n",
            "Epoch 239, Loss: 1.6167\n",
            "Epoch 240, Loss: 1.6037\n",
            "Epoch 241, Loss: 1.6194\n",
            "Epoch 242, Loss: 1.6173\n",
            "Epoch 243, Loss: 1.6103\n",
            "Epoch 244, Loss: 1.6092\n",
            "Epoch 245, Loss: 1.6159\n",
            "Epoch 246, Loss: 1.6061\n",
            "Epoch 247, Loss: 1.6096\n",
            "Epoch 248, Loss: 1.6102\n",
            "Epoch 249, Loss: 1.6131\n",
            "Epoch 250, Loss: 1.6148\n",
            "Epoch 251, Loss: 1.5963\n",
            "Epoch 252, Loss: 1.6075\n",
            "Epoch 253, Loss: 1.6116\n",
            "Epoch 254, Loss: 1.6070\n",
            "Epoch 255, Loss: 1.6099\n",
            "Epoch 256, Loss: 1.5938\n",
            "Epoch 257, Loss: 1.6167\n",
            "Epoch 258, Loss: 1.6013\n",
            "Epoch 259, Loss: 1.6074\n",
            "Epoch 260, Loss: 1.6040\n",
            "Epoch 261, Loss: 1.5954\n",
            "Epoch 262, Loss: 1.6041\n",
            "Epoch 263, Loss: 1.5992\n",
            "Epoch 264, Loss: 1.5923\n",
            "Epoch 265, Loss: 1.6020\n",
            "Epoch 266, Loss: 1.5961\n",
            "Epoch 267, Loss: 1.5966\n",
            "Epoch 268, Loss: 1.5946\n",
            "Epoch 269, Loss: 1.5933\n",
            "Epoch 270, Loss: 1.5955\n",
            "Epoch 271, Loss: 1.5928\n",
            "Epoch 272, Loss: 1.5851\n",
            "Epoch 273, Loss: 1.5909\n",
            "Epoch 274, Loss: 1.5975\n",
            "Epoch 275, Loss: 1.5867\n",
            "Epoch 276, Loss: 1.5887\n",
            "Epoch 277, Loss: 1.5872\n",
            "Epoch 278, Loss: 1.5897\n",
            "Epoch 279, Loss: 1.5923\n",
            "Epoch 280, Loss: 1.5895\n",
            "Epoch 281, Loss: 1.5906\n",
            "Epoch 282, Loss: 1.5812\n",
            "Epoch 283, Loss: 1.5919\n",
            "Epoch 284, Loss: 1.5790\n",
            "Epoch 285, Loss: 1.5792\n",
            "Epoch 286, Loss: 1.5883\n",
            "Epoch 287, Loss: 1.5772\n",
            "Epoch 288, Loss: 1.5834\n",
            "Epoch 289, Loss: 1.5811\n",
            "Epoch 290, Loss: 1.5840\n",
            "Epoch 291, Loss: 1.5731\n",
            "Epoch 292, Loss: 1.5767\n",
            "Epoch 293, Loss: 1.5751\n",
            "Epoch 294, Loss: 1.5746\n",
            "Epoch 295, Loss: 1.5783\n",
            "Epoch 296, Loss: 1.5818\n",
            "Epoch 297, Loss: 1.5795\n",
            "Epoch 298, Loss: 1.5740\n",
            "Epoch 299, Loss: 1.5691\n",
            "Epoch 300, Loss: 1.5725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, path, word2idx, idx2word):\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'word2idx': word2idx,\n",
        "        'idx2word': idx2word,\n",
        "        'hyperparameters': {\n",
        "            'vocab_size': vocab_size,\n",
        "            'embed_dim': embed_dim,\n",
        "            'num_heads': num_heads,\n",
        "            'ff_dim': ff_dim,\n",
        "            'num_layers': num_layers\n",
        "        }\n",
        "    }\n",
        "    torch.save(checkpoint, path)\n",
        "    print(f\"Model saved to {path}\")\n",
        "\n",
        "def load_model(path, device='cpu'):\n",
        "    checkpoint = torch.load(path, map_location=device)\n",
        "    hp = checkpoint['hyperparameters']\n",
        "\n",
        "    model = DenoisingTransformer(\n",
        "        hp['vocab_size'],\n",
        "        hp['embed_dim'],\n",
        "        hp['num_heads'],\n",
        "        hp['ff_dim'],\n",
        "        hp['num_layers']\n",
        "    ).to(device)\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    return model, checkpoint['word2idx'], checkpoint['idx2word']"
      ],
      "metadata": {
        "id": "iw36_xmVTUgD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RMwkATnQ_XE",
        "outputId": "420bf745-b9b5-4cdf-82aa-2d23918fd6ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to denoising_transformer.pth\n"
          ]
        }
      ],
      "source": [
        "save_model(model, 'denoising_transformer.pth', word2idx, idx2word)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "loaded_model, word2idx, idx2word = load_model('denoising_transformer.pth', device=device)\n",
        "\n",
        "print(\"Model loaded successfully!\")\n",
        "print(f\"Model device: {next(loaded_model.parameters()).device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5h_IeiNNW2Pz",
        "outputId": "00e1973e-2266-4d41-ad26-4dfab10c4f09"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n",
            "Model device: cuda:0\n"
          ]
        }
      ]
    }
  ]
}