{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPl0vsaHLVFpa7Cx1LmTdMN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayyucedemirbas/Denoising_Autoencoder_for_Text_Generation/blob/main/char_level_tokenization_Transformer_Based_Denoising_Autoencoder_for_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import requests\n",
        "import string\n",
        "import re\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "s9exHcLt3Mkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "text = requests.get(url).text"
      ],
      "metadata": {
        "id": "GK5sZnmC3P4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text, min_char_freq=3):\n",
        "    text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
        "    chars = list(text)\n",
        "    char_counts = Counter(chars)\n",
        "    vocab = ['<pad>', '<unk>', '<mask>'] + \\\n",
        "            [char for char, count in char_counts.items() if count >= min_char_freq]\n",
        "\n",
        "    char2idx = {char:i for i, char in enumerate(vocab)}\n",
        "    idx2char = {i:char for i, char in enumerate(vocab)}\n",
        "\n",
        "    data = []\n",
        "    for char in chars:\n",
        "        if char in char2idx:\n",
        "            data.append(char2idx[char])\n",
        "        else:\n",
        "            data.append(char2idx['<unk>'])\n",
        "\n",
        "    return data, char2idx, idx2char, len(vocab)\n",
        "\n",
        "data, char2idx, idx2char, vocab_size = preprocess_text(text)"
      ],
      "metadata": {
        "id": "4XMTgWpw3VZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 64\n",
        "batch_size = 32\n",
        "embed_dim = 256\n",
        "num_heads = 4\n",
        "ff_dim = 512\n",
        "num_layers = 3\n",
        "noise_prob = 0.3\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "dC9EYv4v3aEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_batches(data, batch_size, seq_length):\n",
        "    num_batches = len(data) // (batch_size * seq_length)\n",
        "    data = data[:num_batches * batch_size * seq_length]\n",
        "    data = torch.tensor(data).view(batch_size, -1)\n",
        "    return data\n",
        "\n",
        "def add_noise(batch):\n",
        "    device = batch.device\n",
        "    noisy_batch = batch.clone()\n",
        "\n",
        "    mask = torch.rand_like(noisy_batch.float(), device=device) < noise_prob\n",
        "    random_chars = torch.randint(3, vocab_size, noisy_batch.shape, device=device)\n",
        "\n",
        "    newline_mask = (noisy_batch == char2idx.get('\\n', -1))\n",
        "    punctuation_mask = torch.isin(noisy_batch, torch.tensor(\n",
        "        [char2idx[c] for c in string.punctuation if c in char2idx],\n",
        "        device=device\n",
        "    ))\n",
        "    mask[newline_mask | punctuation_mask] &= torch.rand_like(mask.float())[newline_mask | punctuation_mask] < 0.1\n",
        "\n",
        "    noisy_batch[mask] = random_chars[mask]\n",
        "    return noisy_batch"
      ],
      "metadata": {
        "id": "0KRfASCH3dcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
        "        super().__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(ff_dim, embed_dim)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output, _ = self.attention(x, x, x)\n",
        "        x = self.norm1(x + attn_output)\n",
        "        ff_output = self.ff(x)\n",
        "        return self.norm2(x + ff_output)\n",
        "\n",
        "class DenoisingTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, ff_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.positional_encoding = PositionalEncoding(embed_dim)\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, ff_dim) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x)\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "FLZu-jSA3gkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DenoisingTransformer(vocab_size, embed_dim, num_heads, ff_dim, num_layers).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=char2idx['<pad>'])\n",
        "\n",
        "data_tensor = create_batches(data, batch_size, seq_length)\n",
        "num_batches = data_tensor.size(1) // seq_length"
      ],
      "metadata": {
        "id": "krLAhrgi3mAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(300):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        inputs = data_tensor[:, i*seq_length:(i+1)*seq_length].to(device)\n",
        "        noisy_inputs = add_noise(inputs)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(noisy_inputs)\n",
        "        loss = criterion(outputs.view(-1, vocab_size), inputs.view(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        test_text = \"First Citizen:\\nWe are accounted poor citizens...\"\n",
        "        test_chars = list(test_text)\n",
        "        test_data = [char2idx.get(c, char2idx['<unk>']) for c in test_chars]\n",
        "        test_input = torch.tensor(test_data[:seq_length]).unsqueeze(0).to(device)\n",
        "        noisy_test = add_noise(test_input)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            prediction = model(noisy_test).argmax(-1)\n",
        "            restored = ''.join([idx2char[idx.item()] for idx in prediction[0]])\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}\")\n",
        "        print(\"Noisy input:\", ''.join([idx2char.get(idx.item(), '?') for idx in noisy_test[0]]))\n",
        "        print(\"Restored:\", restored)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss/num_batches:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJarL8RJ3sn-",
        "outputId": "11b433cf-d6fa-40e7-b442-456c9b0800e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.3368\n",
            "Epoch 2, Loss: 1.2851\n",
            "Epoch 3, Loss: 1.2847\n",
            "Epoch 4, Loss: 1.2857\n",
            "Epoch 5, Loss: 1.2805\n",
            "Epoch 6, Loss: 1.2851\n",
            "Epoch 7, Loss: 1.2789\n",
            "Epoch 8, Loss: 1.2810\n",
            "Epoch 9, Loss: 1.2817\n",
            "\n",
            "Epoch 10\n",
            "Noisy input: FiEatvCitsren:\n",
            "WeMaaeMacJou ted pooG .idizenX...\n",
            "Restored: FiEatv itsren:\n",
            "We aae ac ou ted poo  .idi en ...\n",
            "Epoch 10, Loss: 1.2811\n",
            "Epoch 11, Loss: 1.2816\n",
            "Epoch 12, Loss: 1.2825\n",
            "Epoch 13, Loss: 1.2810\n",
            "Epoch 14, Loss: 1.2833\n",
            "Epoch 15, Loss: 1.2792\n",
            "Epoch 16, Loss: 1.2772\n",
            "Epoch 17, Loss: 1.2843\n",
            "Epoch 18, Loss: 1.2791\n",
            "Epoch 19, Loss: 1.2810\n",
            "\n",
            "Epoch 20\n",
            "Noisy input: Bjlst Cik3zEn:\n",
            "Ge pri acc!uFted ;oor cktiztn&...\n",
            "Restored:   lst  ik  En:\n",
            " e pri acc u ted ;oor ckti tn ...\n",
            "Epoch 20, Loss: 1.2809\n",
            "Epoch 21, Loss: 1.2812\n",
            "Epoch 22, Loss: 1.2780\n",
            "Epoch 23, Loss: 1.2783\n",
            "Epoch 24, Loss: 1.2807\n",
            "Epoch 25, Loss: 1.2821\n",
            "Epoch 26, Loss: 1.2820\n",
            "Epoch 27, Loss: 1.2810\n",
            "Epoch 28, Loss: 1.2810\n",
            "Epoch 29, Loss: 1.2824\n",
            "\n",
            "Epoch 30\n",
            "Noisy input: j&'se pQdizer:\n",
            "WS?akeNaccounted poor cXRizRns...\n",
            "Restored:   'se p di er:\n",
            "WS akeNaccounted poor c  i  ns...\n",
            "Epoch 30, Loss: 1.2783\n",
            "Epoch 31, Loss: 1.2798\n",
            "Epoch 32, Loss: 1.2790\n",
            "Epoch 33, Loss: 1.2793\n",
            "Epoch 34, Loss: 1.2805\n",
            "Epoch 35, Loss: 1.2791\n",
            "Epoch 36, Loss: 1.2796\n",
            "Epoch 37, Loss: 1.2784\n",
            "Epoch 38, Loss: 1.2765\n",
            "Epoch 39, Loss: 1.2790\n",
            "\n",
            "Epoch 40\n",
            "Noisy input: Firtt WGVizVn:\n",
            "We arV accounted pooI citRzens...\n",
            "Restored:  irtt W  i  n:\n",
            "We ar  accounted pooI citR ens...\n",
            "Epoch 40, Loss: 1.2778\n",
            "Epoch 41, Loss: 1.2785\n",
            "Epoch 42, Loss: 1.2787\n",
            "Epoch 43, Loss: 1.2800\n",
            "Epoch 44, Loss: 1.2820\n",
            "Epoch 45, Loss: 1.2798\n",
            "Epoch 46, Loss: 1.2800\n",
            "Epoch 47, Loss: 1.2783\n",
            "Epoch 48, Loss: 1.2793\n",
            "Epoch 49, Loss: 1.2781\n",
            "\n",
            "Epoch 50\n",
            "Noisy input: Cirst Cfti:eG:\n",
            "WeMare ac:oVnted pAUrfuitczens.-.\n",
            "Restored:  irst  fti:e :\n",
            "We are ac:o nted pAUrfuitc ens. .\n",
            "Epoch 50, Loss: 1.2813\n",
            "Epoch 51, Loss: 1.2817\n",
            "Epoch 52, Loss: 1.2779\n",
            "Epoch 53, Loss: 1.2772\n",
            "Epoch 54, Loss: 1.2800\n",
            "Epoch 55, Loss: 1.2792\n",
            "Epoch 56, Loss: 1.2807\n",
            "Epoch 57, Loss: 1.2780\n",
            "Epoch 58, Loss: 1.2789\n",
            "Epoch 59, Loss: 1.2783\n",
            "\n",
            "Epoch 60\n",
            "Noisy input: FUrJt Citizen:\n",
            "We IQe cccoPntec poorgQiCizenk...\n",
            "Restored:   r t  iti en:\n",
            "We I e ccco ntec poorg i i enk...\n",
            "Epoch 60, Loss: 1.2744\n",
            "Epoch 61, Loss: 1.2816\n",
            "Epoch 62, Loss: 1.2763\n",
            "Epoch 63, Loss: 1.2777\n",
            "Epoch 64, Loss: 1.2770\n",
            "Epoch 65, Loss: 1.2767\n",
            "Epoch 66, Loss: 1.2775\n",
            "Epoch 67, Loss: 1.2788\n",
            "Epoch 68, Loss: 1.2783\n",
            "Epoch 69, Loss: 1.2769\n",
            "\n",
            "Epoch 70\n",
            "Noisy input: Fi?st Cikizen:\n",
            "LACareqaxcounted poor citqzBn;...\n",
            "Restored:  i st  iki en:\n",
            " A are a counted poor cit   n ...\n",
            "Epoch 70, Loss: 1.2776\n",
            "Epoch 71, Loss: 1.2789\n",
            "Epoch 72, Loss: 1.2809\n",
            "Epoch 73, Loss: 1.2762\n",
            "Epoch 74, Loss: 1.2812\n",
            "Epoch 75, Loss: 1.2787\n",
            "Epoch 76, Loss: 1.2784\n",
            "Epoch 77, Loss: 1.2738\n",
            "Epoch 78, Loss: 1.2781\n",
            "Epoch 79, Loss: 1.2796\n",
            "\n",
            "Epoch 80\n",
            "Noisy input: FNrst mHtizen:\n",
            "We WreqaeR!unmfd poor Siti?ens...\n",
            "Restored:  Nrst mHti en:\n",
            "We Wre ae  unmfd poor Siti ens...\n",
            "Epoch 80, Loss: 1.2777\n",
            "Epoch 81, Loss: 1.2757\n",
            "Epoch 82, Loss: 1.2793\n",
            "Epoch 83, Loss: 1.2765\n",
            "Epoch 84, Loss: 1.2792\n",
            "Epoch 85, Loss: 1.2739\n",
            "Epoch 86, Loss: 1.2776\n",
            "Epoch 87, Loss: 1.2766\n",
            "Epoch 88, Loss: 1.2763\n",
            "Epoch 89, Loss: 1.2784\n",
            "\n",
            "Epoch 90\n",
            "Noisy input: FiJst oKtizeW:\n",
            "We are accountVd Oook citizOnl...\n",
            "Restored:  i st o ti eW:\n",
            "We are account d Oook citi Onl...\n",
            "Epoch 90, Loss: 1.2785\n",
            "Epoch 91, Loss: 1.2761\n",
            "Epoch 92, Loss: 1.2772\n",
            "Epoch 93, Loss: 1.2785\n",
            "Epoch 94, Loss: 1.2771\n",
            "Epoch 95, Loss: 1.2802\n",
            "Epoch 96, Loss: 1.2781\n",
            "Epoch 97, Loss: 1.2789\n",
            "Epoch 98, Loss: 1.2749\n",
            "Epoch 99, Loss: 1.2755\n",
            "\n",
            "Epoch 100\n",
            "Noisy input: FirsT IiQiJen:\n",
            "We agP accovnted poor  ivQzeu;...\n",
            "Restored:  irsT Ii i en:\n",
            "We ag  accovnted poor  iv  eu ...\n",
            "Epoch 100, Loss: 1.2782\n",
            "Epoch 101, Loss: 1.2762\n",
            "Epoch 102, Loss: 1.2737\n",
            "Epoch 103, Loss: 1.2736\n",
            "Epoch 104, Loss: 1.2781\n",
            "Epoch 105, Loss: 1.2767\n",
            "Epoch 106, Loss: 1.2769\n",
            "Epoch 107, Loss: 1.2764\n",
            "Epoch 108, Loss: 1.2771\n",
            "Epoch 109, Loss: 1.2783\n",
            "\n",
            "Epoch 110\n",
            "Noisy input: Fifsd 'iOqq-n:\n",
            "beOao! GccoDnted poPItzitdzdnE...\n",
            "Restored:  ifsd 'iO   n:\n",
            "beOao   cco nted po It itd dn    \n",
            "Epoch 110, Loss: 1.2786\n",
            "Epoch 111, Loss: 1.2747\n",
            "Epoch 112, Loss: 1.2774\n",
            "Epoch 113, Loss: 1.2753\n",
            "Epoch 114, Loss: 1.2772\n",
            "Epoch 115, Loss: 1.2760\n",
            "Epoch 116, Loss: 1.2772\n",
            "Epoch 117, Loss: 1.2733\n",
            "Epoch 118, Loss: 1.2764\n",
            "Epoch 119, Loss: 1.2764\n",
            "\n",
            "Epoch 120\n",
            "Noisy input: Firlt CigizeY:\n",
            "WX ar\n",
            "dIcc;GntedXpoon.citizens...\n",
            "Restored:  irlt  igi e :\n",
            "W  ar\n",
            "dIcc  ntedtpoon citi ens   \n",
            "Epoch 120, Loss: 1.2746\n",
            "Epoch 121, Loss: 1.2782\n",
            "Epoch 122, Loss: 1.2744\n",
            "Epoch 123, Loss: 1.2772\n",
            "Epoch 124, Loss: 1.2763\n",
            "Epoch 125, Loss: 1.2748\n",
            "Epoch 126, Loss: 1.2761\n",
            "Epoch 127, Loss: 1.2749\n",
            "Epoch 128, Loss: 1.2752\n",
            "Epoch 129, Loss: 1.2730\n",
            "\n",
            "Epoch 130\n",
            "Noisy input: acrst CitAzen:\n",
            "We areyJccou&tAdtYoor3ciBizens...\n",
            "Restored: acrst  itA en:\n",
            " e arey ccouttAdt oor ci i ens   \n",
            "Epoch 130, Loss: 1.2736\n",
            "Epoch 131, Loss: 1.2734\n",
            "Epoch 132, Loss: 1.2768\n",
            "Epoch 133, Loss: 1.2760\n",
            "Epoch 134, Loss: 1.2755\n",
            "Epoch 135, Loss: 1.2724\n",
            "Epoch 136, Loss: 1.2769\n",
            "Epoch 137, Loss: 1.2743\n",
            "Epoch 138, Loss: 1.2749\n",
            "Epoch 139, Loss: 1.2739\n",
            "\n",
            "Epoch 140\n",
            "Noisy input: FiPUt Citihen:\n",
            "We are accfjnHGd poor cetnIeEs...\n",
            "Restored:  i  t  itihen \n",
            " e are accf n  d poor cetnIeXs...\n",
            "Epoch 140, Loss: 1.2735\n",
            "Epoch 141, Loss: 1.2761\n",
            "Epoch 142, Loss: 1.2760\n",
            "Epoch 143, Loss: 1.2754\n",
            "Epoch 144, Loss: 1.2757\n",
            "Epoch 145, Loss: 1.2753\n",
            "Epoch 146, Loss: 1.2740\n",
            "Epoch 147, Loss: 1.2733\n",
            "Epoch 148, Loss: 1.2771\n",
            "Epoch 149, Loss: 1.2730\n",
            "\n",
            "Epoch 150\n",
            "Noisy input: First Cftpzen:\n",
            "LeoaRH aPcHujtnd poor FitiMBns...\n",
            "Restored:  irst  ftpVen \n",
            " eoa   a c u tnd poor  iti Bns...\n",
            "Epoch 150, Loss: 1.2752\n",
            "Epoch 151, Loss: 1.2726\n",
            "Epoch 152, Loss: 1.2721\n",
            "Epoch 153, Loss: 1.2770\n",
            "Epoch 154, Loss: 1.2732\n",
            "Epoch 155, Loss: 1.2753\n",
            "Epoch 156, Loss: 1.2727\n",
            "Epoch 157, Loss: 1.2734\n",
            "Epoch 158, Loss: 1.2739\n",
            "Epoch 159, Loss: 1.2732\n",
            "\n",
            "Epoch 160\n",
            "Noisy input: FirsD CibizeA:\n",
            "We Gre accounteU oo,r ciyize:s...\n",
            "Restored: airs   i i eA \n",
            " e  re accounte  oo,r ciyi e s.. \n",
            "Epoch 160, Loss: 1.2720\n",
            "Epoch 161, Loss: 1.2735\n",
            "Epoch 162, Loss: 1.2703\n",
            "Epoch 163, Loss: 1.2736\n",
            "Epoch 164, Loss: 1.2738\n",
            "Epoch 165, Loss: 1.2729\n",
            "Epoch 166, Loss: 1.2714\n",
            "Epoch 167, Loss: 1.2728\n",
            "Epoch 168, Loss: 1.2741\n",
            "Epoch 169, Loss: 1.2715\n",
            "\n",
            "Epoch 170\n",
            "Noisy input: Fi!k:ICitizzn:\n",
            "We arflaccSkpter Vooa citizbns...\n",
            "Restored:  i k:I iti  nx\n",
            "We ar lac  kpter  ooa citiJbns...\n",
            "Epoch 170, Loss: 1.2745\n",
            "Epoch 171, Loss: 1.2701\n",
            "Epoch 172, Loss: 1.2725\n",
            "Epoch 173, Loss: 1.2746\n",
            "Epoch 174, Loss: 1.2713\n",
            "Epoch 175, Loss: 1.2734\n",
            "Epoch 176, Loss: 1.2729\n",
            "Epoch 177, Loss: 1.2722\n",
            "Epoch 178, Loss: 1.2715\n",
            "Epoch 179, Loss: 1.2706\n",
            "\n",
            "Epoch 180\n",
            "Noisy input: Firse vitize?:\n",
            "WL are acUSuTted p-or;citizeDs...\n",
            "Restored: xirse viti e  \n",
            "   are ac  uTted p Vr citi e s   \n",
            "Epoch 180, Loss: 1.2715\n",
            "Epoch 181, Loss: 1.2757\n",
            "Epoch 182, Loss: 1.2727\n",
            "Epoch 183, Loss: 1.2722\n",
            "Epoch 184, Loss: 1.2700\n",
            "Epoch 185, Loss: 1.2735\n",
            "Epoch 186, Loss: 1.2744\n",
            "Epoch 187, Loss: 1.2711\n",
            "Epoch 188, Loss: 1.2725\n",
            "Epoch 189, Loss: 1.2730\n",
            "\n",
            "Epoch 190\n",
            "Noisy input: fikst Iitizen:\n",
            "W; areMnccaunZed poor citi,enL..z\n",
            "Restored:  i st Iiti en \n",
            "   are ncca n ed poor c t  en    \n",
            "Epoch 190, Loss: 1.2749\n",
            "Epoch 191, Loss: 1.2727\n",
            "Epoch 192, Loss: 1.2709\n",
            "Epoch 193, Loss: 1.2759\n",
            "Epoch 194, Loss: 1.2742\n",
            "Epoch 195, Loss: 1.2710\n",
            "Epoch 196, Loss: 1.2749\n",
            "Epoch 197, Loss: 1.2728\n",
            "Epoch 198, Loss: 1.2732\n",
            "Epoch 199, Loss: 1.2743\n",
            "\n",
            "Epoch 200\n",
            "Noisy input: Fivst CitizX;:\n",
            "Me are wYcyuntedYpoor cutiqens...\n",
            "Restored:  ivst  iti  ; \n",
            " e are w c  nted poor c t  ens   \n",
            "Epoch 200, Loss: 1.2707\n",
            "Epoch 201, Loss: 1.2704\n",
            "Epoch 202, Loss: 1.2749\n",
            "Epoch 203, Loss: 1.2709\n",
            "Epoch 204, Loss: 1.2725\n",
            "Epoch 205, Loss: 1.2704\n",
            "Epoch 206, Loss: 1.2729\n",
            "Epoch 207, Loss: 1.2721\n",
            "Epoch 208, Loss: 1.2685\n",
            "Epoch 209, Loss: 1.2708\n",
            "\n",
            "Epoch 210\n",
            "Noisy input: F?rst CitiZen:\n",
            "Qeoare aacrun:edaQoor citizens...\n",
            "Restored:   rst  iti en <unk> eoare aacr n eda oor cit  ens   \n",
            "Epoch 210, Loss: 1.2724\n",
            "Epoch 211, Loss: 1.2731\n",
            "Epoch 212, Loss: 1.2705\n",
            "Epoch 213, Loss: 1.2715\n",
            "Epoch 214, Loss: 1.2737\n",
            "Epoch 215, Loss: 1.2691\n",
            "Epoch 216, Loss: 1.2690\n",
            "Epoch 217, Loss: 1.2745\n",
            "Epoch 218, Loss: 1.2710\n",
            "Epoch 219, Loss: 1.2688\n",
            "\n",
            "Epoch 220\n",
            "Noisy input: First Citi;Cb:\n",
            "Jeqare Vccoun-eQ poor qutizens...\n",
            "Restored:  irst   ti H  \n",
            " e are  c o n e  poor   ti ens   \n",
            "Epoch 220, Loss: 1.2717\n",
            "Epoch 221, Loss: 1.2717\n",
            "Epoch 222, Loss: 1.2707\n",
            "Epoch 223, Loss: 1.2687\n",
            "Epoch 224, Loss: 1.2717\n",
            "Epoch 225, Loss: 1.2690\n",
            "Epoch 226, Loss: 1.2717\n",
            "Epoch 227, Loss: 1.2701\n",
            "Epoch 228, Loss: 1.2706\n",
            "Epoch 229, Loss: 1.2688\n",
            "\n",
            "Epoch 230\n",
            "Noisy input: H Kst zLtHzeS:\n",
            "Wem'rt Lccounted pIIrVcit&L ns...\n",
            "Restored:   Kst G t  eS   em rt  cco nte  Fl r  it   ns.  \n",
            "Epoch 230, Loss: 1.2701\n",
            "Epoch 231, Loss: 1.2699\n",
            "Epoch 232, Loss: 1.2667\n",
            "Epoch 233, Loss: 1.2698\n",
            "Epoch 234, Loss: 1.2711\n",
            "Epoch 235, Loss: 1.2682\n",
            "Epoch 236, Loss: 1.2695\n",
            "Epoch 237, Loss: 1.2687\n",
            "Epoch 238, Loss: 1.2685\n",
            "Epoch 239, Loss: 1.2707\n",
            "\n",
            "Epoch 240\n",
            "Noisy input: FirLF Citpzeq:\n",
            "WI areZacaoEnted poor cjt,zecs...\n",
            "Restored: j r     t  e  \n",
            "   are a aoanteM  oor  Vt  eVs   \n",
            "Epoch 240, Loss: 1.2696\n",
            "Epoch 241, Loss: 1.2678\n",
            "Epoch 242, Loss: 1.2693\n",
            "Epoch 243, Loss: 1.2667\n",
            "Epoch 244, Loss: 1.2677\n",
            "Epoch 245, Loss: 1.2675\n",
            "Epoch 246, Loss: 1.2694\n",
            "Epoch 247, Loss: 1.2663\n",
            "Epoch 248, Loss: 1.2696\n",
            "Epoch 249, Loss: 1.2699\n",
            "\n",
            "Epoch 250\n",
            "Noisy input: First Citieln:\n",
            "We aTw Mccounted &RofRcitizePs...\n",
            "Restored: j rs  H  WeRn <unk>We a   Kcc  nt    Voi  3 W e s   \n",
            "Epoch 250, Loss: 1.2675\n",
            "Epoch 251, Loss: 1.2668\n",
            "Epoch 252, Loss: 1.2669\n",
            "Epoch 253, Loss: 1.2686\n",
            "Epoch 254, Loss: 1.2672\n",
            "Epoch 255, Loss: 1.2683\n",
            "Epoch 256, Loss: 1.2663\n",
            "Epoch 257, Loss: 1.2694\n",
            "Epoch 258, Loss: 1.2674\n",
            "Epoch 259, Loss: 1.2729\n",
            "\n",
            "Epoch 260\n",
            "Noisy input: Flrst CitozeQ:\n",
            "WWfare acco;nIedmpo\n",
            "r !it'zeFs...\n",
            "Restored:  Yrst      e x<unk>W  a M aY   n?e fno<unk>r   t  e s   \n",
            "Epoch 260, Loss: 1.2676\n",
            "Epoch 261, Loss: 1.2668\n",
            "Epoch 262, Loss: 1.2624\n",
            "Epoch 263, Loss: 1.2657\n",
            "Epoch 264, Loss: 1.2681\n",
            "Epoch 265, Loss: 1.2671\n",
            "Epoch 266, Loss: 1.2668\n",
            "Epoch 267, Loss: 1.2658\n",
            "Epoch 268, Loss: 1.2659\n",
            "Epoch 269, Loss: 1.2712\n",
            "\n",
            "Epoch 270\n",
            "Noisy input: N!rse Citi;en:\n",
            "Wq !rO Occuunted ? arFc3tizXns.b.\n",
            "Restored:  Yrs  OeqW  nD W   r        t      rxN tW  ns - \n",
            "Epoch 270, Loss: 1.2675\n",
            "Epoch 271, Loss: 1.2699\n",
            "Epoch 272, Loss: 1.2688\n",
            "Epoch 273, Loss: 1.2693\n",
            "Epoch 274, Loss: 1.2665\n",
            "Epoch 275, Loss: 1.2631\n",
            "Epoch 276, Loss: 1.2624\n",
            "Epoch 277, Loss: 1.2646\n",
            "Epoch 278, Loss: 1.2631\n",
            "Epoch 279, Loss: 1.2643\n",
            "\n",
            "Epoch 280\n",
            "Noisy input: FFrst Cqtizen:\n",
            "We are acrountedPpoor cBtize?s...\n",
            "Restored: x rst   t  enD  e a e aaYo nte   oor  Mt  e s   \n",
            "Epoch 280, Loss: 1.2668\n",
            "Epoch 281, Loss: 1.2656\n",
            "Epoch 282, Loss: 1.2647\n",
            "Epoch 283, Loss: 1.2650\n",
            "Epoch 284, Loss: 1.2640\n",
            "Epoch 285, Loss: 1.2667\n",
            "Epoch 286, Loss: 1.2655\n",
            "Epoch 287, Loss: 1.2640\n",
            "Epoch 288, Loss: 1.2647\n",
            "Epoch 289, Loss: 1.2647\n",
            "\n",
            "Epoch 290\n",
            "Noisy input: First ditize::\n",
            "We aGecUcvoVhted :oorycitizeHs...\n",
            "Restored: x rst \n",
            " t  e VMWe a e    o  te  Door   t  e s   \n",
            "Epoch 290, Loss: 1.2651\n",
            "Epoch 291, Loss: 1.2623\n",
            "Epoch 292, Loss: 1.2629\n",
            "Epoch 293, Loss: 1.2609\n",
            "Epoch 294, Loss: 1.2656\n",
            "Epoch 295, Loss: 1.2633\n",
            "Epoch 296, Loss: 1.2649\n",
            "Epoch 297, Loss: 1.2675\n",
            "Epoch 298, Loss: 1.2652\n",
            "Epoch 299, Loss: 1.2626\n",
            "\n",
            "Epoch 300\n",
            "Noisy input: FLust Citizen:\n",
            "WeOPrAskccountedRpoor citizd;s...\n",
            "Restored: x  st   t  enx WeOV As   o WteM Uoor   t    s   \n",
            "Epoch 300, Loss: 1.2635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, path, char2idx, idx2char):\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'char2idx': char2idx,\n",
        "        'idx2char': idx2char,\n",
        "        'hyperparameters': {\n",
        "            'vocab_size': vocab_size,\n",
        "            'embed_dim': embed_dim,\n",
        "            'num_heads': num_heads,\n",
        "            'ff_dim': ff_dim,\n",
        "            'num_layers': num_layers\n",
        "        }\n",
        "    }\n",
        "    torch.save(checkpoint, path)\n",
        "    print(f\"Model saved to {path}\")\n",
        "\n",
        "def load_model(path, device='cpu'):\n",
        "    checkpoint = torch.load(path, map_location=device)\n",
        "    hp = checkpoint['hyperparameters']\n",
        "\n",
        "    model = DenoisingTransformer(\n",
        "        hp['vocab_size'],\n",
        "        hp['embed_dim'],\n",
        "        hp['num_heads'],\n",
        "        hp['ff_dim'],\n",
        "        hp['num_layers']\n",
        "    ).to(device)\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    return model, checkpoint['char2idx'], checkpoint['idx2char']"
      ],
      "metadata": {
        "id": "RKsRF5QC3wwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_model(model, 'char_denoising_transformer.pth', char2idx, idx2char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJj5L3Sp33_G",
        "outputId": "cfb51725-339d-41e6-e9f9-0d4b0c2ad704"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to char_denoising_transformer.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model, char2idx, idx2char = load_model('char_denoising_transformer.pth', device=device)"
      ],
      "metadata": {
        "id": "bsV3NYBY35Ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_phrase = \"\\nSecond Citizen:\\nLet us kill him!\\n\"\n",
        "test_data = [char2idx.get(c, char2idx['<unk>']) for c in test_phrase]\n",
        "test_input = torch.tensor(test_data).unsqueeze(0).to(device)\n",
        "noisy_test = add_noise(test_input)\n",
        "\n",
        "with torch.no_grad():\n",
        "    prediction = model(noisy_test).argmax(-1)\n",
        "    restored = ''.join([idx2char[idx.item()] for idx in prediction[0]])\n",
        "\n",
        "print(\"Original:\", test_phrase)\n",
        "print(\"Noisy:   \", ''.join([idx2char.get(idx.item(), '?') for idx in noisy_test[0]]))\n",
        "print(\"Restored:\", restored)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQgQXqJC4G3I",
        "outputId": "e6ee7603-d2bf-473f-e741-2751765077e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: \n",
            "Second Citizen:\n",
            "Let us kill him!\n",
            "\n",
            "Noisy:    \n",
            "veaondqCitizen:\n",
            "LeD uE kilA him!\n",
            "\n",
            "Restored:  ve onMGV t  enD  e        A h    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48C0BDPQ3CYC"
      },
      "outputs": [],
      "source": [
        "test_phrase = \"\\nSecond Citizen:\\nLet us kill him!\\n\"\n",
        "test_data = [char2idx.get(c, char2idx['<unk>']) for c in test_phrase]\n",
        "test_input = torch.tensor(test_data).unsqueeze(0).to(device)\n",
        "noisy_test = add_noise(test_input)\n",
        "\n",
        "with torch.no_grad():\n",
        "    prediction = loaded_model(noisy_test).argmax(-1)\n",
        "    restored = ''.join([idx2char[idx.item()] for idx in prediction[0]])\n",
        "\n",
        "print(\"Original:\", test_phrase)\n",
        "print(\"Noisy:   \", ''.join([idx2char.get(idx.item(), '?') for idx in noisy_test[0]]))\n",
        "print(\"Restored:\", restored)"
      ]
    }
  ]
}